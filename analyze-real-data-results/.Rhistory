sum(tally_parents$n_rows[-1])
tally_parents$n_rows[-1]
den
num
n_one_only+n_three_or_more+7063
# split parents (for those with two parental lineages)
tmp <- res_filtered %>%
separate(parental_lineages, into = c("pA", "pB"), sep = ";", remove = FALSE)
# how many rows have "other" as a parent
n_with_other <- tmp %>%
filter(str_to_lower(pA) == "other" | str_to_lower(pB) == "other") %>%
nrow()
# drop rows with "other", canonicalize ordering, and tally
pair_counts <- tmp %>%
filter(str_to_lower(pA) != "other", str_to_lower(pB) != "other") %>%
mutate(parent1 = pmin(pA, pB), parent2 = pmax(pA, pB)) %>%
count(parent1, parent2, name = "n_sequences") %>%
arrange(desc(n_sequences), parent1, parent2)
# pair_counts has the number of detected recombinants with each unique parental lineage pair
# drop rows with "other", canonicalize ordering, and tally
pair_counts_w_other <- tmp %>%
mutate(parent1 = pmin(pA, pB), parent2 = pmax(pA, pB)) %>%
count(parent1, parent2, test_start, name = "n_sequences") %>%
arrange(test_start, parent1, parent2)
library(tidyverse)
library(stringr)
library(readr)
library(purrr)
library(data.table)
# Inferred
res <- read.csv("summary_files/combined_optimization_results_with_summary.csv") %>%
mutate(across(c(test_start, test_end), as.Date)) %>%
mutate(date = test_start + (test_end - test_start) / 2)
# add number of parental lineages
res_w_n_parents <- res %>%
mutate(
n_parents = case_when(
is.na(parental_lineages) | parental_lineages == "" ~ 0L,
TRUE ~ str_count(parental_lineages, fixed(";")) + 1L
)
)
# counts
n_three_or_more <- sum(res_w_n_parents$n_parents >= 3, na.rm = TRUE)
n_one_only      <- sum(res_w_n_parents$n_parents == 1, na.rm = TRUE)
# filtered data: keep exactly 2 parental lineages
res_filtered <- res_w_n_parents %>%
filter(n_parents == 2) %>%
select(-n_parents)
# (optional) quick sanity table
tally_parents <- res_w_n_parents %>%
count(n_parents, name = "n_rows") %>%
arrange(n_parents)
# Compute numerator and denominator
num <- nrow(res_filtered)
den <- sum(tally_parents$n_rows[-1])
# Exact (Clopper–Pearson) binomial CI
bt <- binom.test(num, den)
bt$estimate   # estimated proportion
bt$conf.int   # exact 95% Clopper–Pearson CI
# split parents (for those with two parental lineages)
tmp <- res_filtered %>%
separate(parental_lineages, into = c("pA", "pB"), sep = ";", remove = FALSE)
# how many rows have "other" as a parent
n_with_other <- tmp %>%
filter(str_to_lower(pA) == "other" | str_to_lower(pB) == "other") %>%
nrow()
# drop rows with "other", canonicalize ordering, and tally
pair_counts <- tmp %>%
filter(str_to_lower(pA) != "other", str_to_lower(pB) != "other") %>%
mutate(parent1 = pmin(pA, pB), parent2 = pmax(pA, pB)) %>%
count(parent1, parent2, name = "n_sequences") %>%
arrange(desc(n_sequences), parent1, parent2)
# pair_counts has the number of detected recombinants with each unique parental lineage pair
# with "other", canonicalize ordering, and tally
pair_counts_w_other <- tmp %>%
mutate(parent1 = pmin(pA, pB), parent2 = pmax(pA, pB)) %>%
count(parent1, parent2, test_start, name = "n_sequences") %>%
arrange(test_start, parent1, parent2)
### Above, we already obtained the observed counts. However, at this stage, we haven't filtered to the ONS period, but we have excluded other.
### From here, we first get ONS estimates averaged within each window
# Get number of samples and number of detected recombinants with two parental lineages in each window
res_pairs <- res %>%
add_count(date, name = "n_samples") %>%          # <- adds n_samples per date
filter(str_count(parental_lineages, ";") == 1) %>%
group_by(date, test_start, test_end, n_samples) %>%
summarise(n_inferred = n(), .groups = "drop")
sum(res_pairs$n_inferred)
# Get test windows
windows <- res_pairs %>% select(test_start, test_end)
# Read and prepare ONS data
ons <- read.table("../data/ons_survey_data.tsv")
ons <- ons %>%
transmute(
day   = V1, month_name = V2, year = V3, percent = V4,
prop  = percent / 100,
date  = as.Date(paste(year, month_name, day, sep = "-"), format = "%Y-%B-%d")
)
# Expand windows to daily dates, join, and summarize
# The first four lines are left joining ons prevalence to expanded windows (now we have window and day as each row)
ans2 <- windows %>%
mutate(date = map2(test_start, test_end, ~ seq(.x, .y, by = "day"))) %>%
unnest(date) %>%
left_join(ons, by = "date") %>%
group_by(test_start, test_end) %>%
summarise(
avg_prop = mean(prop, na.rm = TRUE),
n_days   = sum(!is.na(prop)),
.groups  = "drop")
### Above, we obtained window-averaged ONS estimates. Next, we need lineage proportions.
# folder containing the files (adjust as needed)
dir_in <- "../run-on-cluster/real-data-analysis/output/sliding_windows/expected_recombinant_freq"
# regex for filenames like: lineage_freq_2024-02-19_2024-02-25.csv.gz
pat <- "^lineage_freq_(\\d{4}-\\d{2}-\\d{2})_(\\d{4}-\\d{2}-\\d{2})\\.csv\\.gz$"
file_df <- tibble(
path = list.files(dir_in, pattern = pat, full.names = TRUE)) %>%
mutate(
fname      = basename(path),
test_start = as.Date(str_match(fname, pat)[, 2]),
test_end   = as.Date(str_match(fname, pat)[, 3]))
# (optional) sanity check
stopifnot(all(!is.na(file_df$test_start)), all(!is.na(file_df$test_end)))
lineage_freq_all <- file_df %>%
select(path, test_start, test_end) %>%
pmap_dfr(function(path, test_start, test_end) {
read_csv(path, show_col_types = FALSE) %>%
mutate(test_start = test_start,
test_end   = test_end)})
# Result: one big data frame with the original columns + test_start/test_end
lineage_freq_all
# Here, we are obtaining sum (pipj) across all i<j, which is what we need to fit the linear regression later
# Windows can include periods when ONS estimates are not available
pair_mass <- lineage_freq_all %>%
group_by(test_start, test_end) %>%
summarise(
s1 = sum(p, na.rm = TRUE),
s2 = sum(p^2, na.rm = TRUE),
sum_pipj = 0.5 * (s1^2 - s2),
.groups = "drop"
)
# Sanity check
pair_mass2 <- lineage_freq_all %>%
group_by(test_start, test_end) %>%
summarise(
sum_pipj = {
v <- p[!is.na(p)]
if (length(v) < 2) 0 else sum(combn(v, 2, FUN = prod))
},
.groups = "drop"
)
pair_mass
pair_mass2
View(pair_mass)
View(lineage_freq_all)
View(res_pairs)
# Using lineage proportions + ONS prev estimates, we next estimate theta and phi using a linear regression.
# To the inferred number of recombinant sequences in each window, left join sum pipj in each window and the window-averaged ons prev.
window_stats <- res_pairs %>% left_join(pair_mass) %>% left_join(ans2)
# window_stats$denom <- window_stats$n_samples*window_stats$avg_prop*window_stats$sum_pipj
# window_stats$theta_first_model <- window_stats$n_inferred/window_stats$denom
# x_w = prev(w) sum pi(w)pj(w)
window_stats$x <- window_stats$avg_prop*window_stats$sum_pipj
# y_w = R^{total}(w)/n(w)
window_stats$y <- window_stats$n_inferred/window_stats$n_samples
window_stats <- window_stats[!is.na(window_stats$x),]
ggplot(window_stats, aes(x, y)) + geom_point()
# linear regression to est. theta and phi
lm <- lm(y~x, data = window_stats)
summary_lm <- summary(lm)
vc <- vcov(lm)
# Point estimates
phi_hat   <- summary_lm$coefficients[1, 1]
beta_hat  <- summary_lm$coefficients[2, 1]
theta_hat <- phi_hat + beta_hat
# Standard errors
se_phi   <- sqrt(vc[1, 1])
se_theta <- sqrt(vc[1, 1] + vc[2, 2] + 2 * vc[1, 2])
# Critical value (e.g., 95% CI)
alpha <- 0.05
z_crit <- qnorm(1 - alpha / 2)
# Wald CIs
phi_CI   <- c(phi_hat - z_crit * se_phi,   phi_hat + z_crit * se_phi)
theta_CI <- c(theta_hat - z_crit * se_theta, theta_hat + z_crit * se_theta)
phi_hat
phi_CI
theta_hat
theta_CI
View(window_stats)
43/2872
vc[2, 2]
vc[1, 1]
vc[2, 1]
vc[1, 2]
### Finally, we are ready to estimate recombinant counts
lineage_freq_all
# Get pipj for all i<j and all windows (using self-join with window id)
pairs_by_window <- lineage_freq_all %>%
filter(test_end <= as.Date("2023-03-19")) %>%
transmute(test_start, test_end, lineage = collapsed, p) %>%
group_by(test_start, test_end) %>%
mutate(.row = row_number()) %>%
ungroup() %>%                                         # <-- important: ungroup before self-join
inner_join(., ., by = c("test_start", "test_end"),
suffix = c("_i", "_j")) %>%
filter(.row_i < .row_j) %>%
transmute(
test_start, test_end,
i_raw  = lineage_i, j_raw  = lineage_j,
pi_raw = p_i,       pj_raw = p_j
) %>%
mutate(
i  = pmin(i_raw, j_raw),
j  = pmax(i_raw, j_raw),
pi = if_else(i_raw == i,  pi_raw, pj_raw),  # swap p’s if we swapped labels
pj = if_else(i_raw == i,  pj_raw, pi_raw),
pipj = pi * pj
) %>%
select(test_start, test_end, i, j, pi, pj, pipj)
# Attach theta, prev, and n
calculate_exp <- pairs_by_window %>% left_join(window_stats %>% select(test_start, test_end, n_samples, avg_prop))
calculate_exp$theta_hat <- theta_hat
# \hat E[R] = theta*n*prev*pi*pj (in each window)
calculate_exp$exp_hat <- calculate_exp$theta_hat * calculate_exp$n_samples * calculate_exp$avg_prop * calculate_exp$pipj
# sum across windows
calculate_exp_agg <- calculate_exp %>% group_by(i,j) %>% summarize(exp_hat = sum(exp_hat)) %>% arrange(desc(exp_hat))
colnames(calculate_exp_agg) <- c("parent1", "parent2", "exp")
pairs_by_window
# sanity check
pairs_by_window %>% group_by(test_start) %>% summarize(sum(pipj))
# sanity check
pairs_by_window %>% group_by(test_start) %>% summarize(sum(pipj))
# sanity check
test <- pairs_by_window %>% group_by(test_start) %>% summarize(sum(pipj))
View(test)
View(lineage_freq_all)
View(pair_mass)
View(pairs_by_window)
View(tmp)
pairs_by_windowsum(pair_counts_w_exp$exp)
sum(pair_counts_w_exp$exp)
library(tidyverse)
library(stringr)
library(readr)
library(purrr)
library(data.table)
# Inferred
res <- read.csv("summary_files/combined_optimization_results_with_summary.csv") %>%
mutate(across(c(test_start, test_end), as.Date)) %>%
mutate(date = test_start + (test_end - test_start) / 2)
# add number of parental lineages
res_w_n_parents <- res %>%
mutate(
n_parents = case_when(
is.na(parental_lineages) | parental_lineages == "" ~ 0L,
TRUE ~ str_count(parental_lineages, fixed(";")) + 1L
)
)
# counts
n_three_or_more <- sum(res_w_n_parents$n_parents >= 3, na.rm = TRUE)
n_one_only      <- sum(res_w_n_parents$n_parents == 1, na.rm = TRUE)
# filtered data: keep exactly 2 parental lineages
res_filtered <- res_w_n_parents %>%
filter(n_parents == 2) %>%
select(-n_parents)
# (optional) quick sanity table
tally_parents <- res_w_n_parents %>%
count(n_parents, name = "n_rows") %>%
arrange(n_parents)
# Compute numerator and denominator
num <- nrow(res_filtered)
den <- sum(tally_parents$n_rows[-1])
# Exact (Clopper–Pearson) binomial CI
bt <- binom.test(num, den)
bt$estimate   # estimated proportion
bt$conf.int   # exact 95% Clopper–Pearson CI
# split parents (for those with two parental lineages)
tmp <- res_filtered %>%
separate(parental_lineages, into = c("pA", "pB"), sep = ";", remove = FALSE)
# how many rows have "other" as a parent
n_with_other <- tmp %>%
filter(str_to_lower(pA) == "other" | str_to_lower(pB) == "other") %>%
nrow()
# drop rows with "other", canonicalize ordering, and tally
pair_counts <- tmp %>%
filter(str_to_lower(pA) != "other", str_to_lower(pB) != "other") %>%
mutate(parent1 = pmin(pA, pB), parent2 = pmax(pA, pB)) %>%
count(parent1, parent2, name = "n_sequences") %>%
arrange(desc(n_sequences), parent1, parent2)
# pair_counts has the number of detected recombinants with each unique parental lineage pair
# with "other", canonicalize ordering, and tally
pair_counts_w_other <- tmp %>%
mutate(parent1 = pmin(pA, pB), parent2 = pmax(pA, pB)) %>%
count(parent1, parent2, test_start, name = "n_sequences") %>%
arrange(test_start, parent1, parent2)
### Above, we already obtained the observed counts. However, at this stage, we haven't filtered to the ONS period, but we have excluded other.
### From here, we first get ONS estimates averaged within each window
# Get number of samples and number of detected recombinants with two parental lineages in each window
res_pairs <- res %>%
add_count(date, name = "n_samples") %>%          # <- adds n_samples per date
filter(str_count(parental_lineages, ";") == 1) %>%
group_by(date, test_start, test_end, n_samples) %>%
summarise(n_inferred = n(), .groups = "drop")
sum(res_pairs$n_inferred)
# Get test windows
windows <- res_pairs %>% select(test_start, test_end)
# Read and prepare ONS data
ons <- read.table("../data/ons_survey_data.tsv")
ons <- ons %>%
transmute(
day   = V1, month_name = V2, year = V3, percent = V4,
prop  = percent / 100,
date  = as.Date(paste(year, month_name, day, sep = "-"), format = "%Y-%B-%d")
)
# Expand windows to daily dates, join, and summarize
# The first four lines are left joining ons prevalence to expanded windows (now we have window and day as each row)
ans2 <- windows %>%
mutate(date = map2(test_start, test_end, ~ seq(.x, .y, by = "day"))) %>%
unnest(date) %>%
left_join(ons, by = "date") %>%
group_by(test_start, test_end) %>%
summarise(
avg_prop = mean(prop, na.rm = TRUE),
n_days   = sum(!is.na(prop)),
.groups  = "drop")
### Above, we obtained window-averaged ONS estimates. Next, we need lineage proportions.
# folder containing the files (adjust as needed)
dir_in <- "../run-on-cluster/real-data-analysis/output/sliding_windows/expected_recombinant_freq"
# regex for filenames like: lineage_freq_2024-02-19_2024-02-25.csv.gz
pat <- "^lineage_freq_(\\d{4}-\\d{2}-\\d{2})_(\\d{4}-\\d{2}-\\d{2})\\.csv\\.gz$"
file_df <- tibble(
path = list.files(dir_in, pattern = pat, full.names = TRUE)) %>%
mutate(
fname      = basename(path),
test_start = as.Date(str_match(fname, pat)[, 2]),
test_end   = as.Date(str_match(fname, pat)[, 3]))
# (optional) sanity check
stopifnot(all(!is.na(file_df$test_start)), all(!is.na(file_df$test_end)))
lineage_freq_all <- file_df %>%
select(path, test_start, test_end) %>%
pmap_dfr(function(path, test_start, test_end) {
read_csv(path, show_col_types = FALSE) %>%
mutate(test_start = test_start,
test_end   = test_end)})
# Result: one big data frame with the original columns + test_start/test_end
lineage_freq_all
# Here, we are obtaining sum (pipj) across all i<j, which is what we need to fit the linear regression later
# Windows can include periods when ONS estimates are not available (will left join later)
# Here we are intentionally including other
pair_mass <- lineage_freq_all %>%
group_by(test_start, test_end) %>%
summarise(
s1 = sum(p, na.rm = TRUE),
s2 = sum(p^2, na.rm = TRUE),
sum_pipj = 0.5 * (s1^2 - s2),
.groups = "drop"
)
# Sanity check
pair_mass2 <- lineage_freq_all %>%
group_by(test_start, test_end) %>%
summarise(
sum_pipj = {
v <- p[!is.na(p)]
if (length(v) < 2) 0 else sum(combn(v, 2, FUN = prod))
},
.groups = "drop"
)
pair_mass
pair_mass2
# plot frequency of other
lineage_freq_all_other <- lineage_freq_all %>% filter(collapsed == "other")
ggplot(lineage_freq_all_other, aes(x = test_start, y = p)) + geom_line()
# test
# pairs_coexist <- lineage_freq_all %>%
#   group_by(test_start) %>%
#   summarise(lineages = list(unique(collapsed)), .groups = "drop")
#
# lapply(pairs_coexist$lineages, function(x){"AY.9" %in% x & "B.1.617.2" %in% x})
# Using lineage proportions + ONS prev estimates, we next estimate theta and phi using a linear regression.
# To the inferred number of recombinant sequences in each window, left join sum pipj in each window and the window-averaged ons prev.
window_stats <- res_pairs %>% left_join(pair_mass) %>% left_join(ans2)
# window_stats$denom <- window_stats$n_samples*window_stats$avg_prop*window_stats$sum_pipj
# window_stats$theta_first_model <- window_stats$n_inferred/window_stats$denom
# x_w = prev(w) sum pi(w)pj(w)
window_stats$x <- window_stats$avg_prop*window_stats$sum_pipj
# y_w = R^{total}(w)/n(w)
window_stats$y <- window_stats$n_inferred/window_stats$n_samples
window_stats <- window_stats[!is.na(window_stats$x),]
ggplot(window_stats, aes(x, y)) + geom_point()
# linear regression to est. theta and phi
lm <- lm(y~x, data = window_stats)
summary_lm <- summary(lm)
vc <- vcov(lm)
# Point estimates
phi_hat   <- summary_lm$coefficients[1, 1]
beta_hat  <- summary_lm$coefficients[2, 1]
theta_hat <- phi_hat + beta_hat
# Standard errors
se_phi   <- sqrt(vc[1, 1])
se_theta <- sqrt(vc[1, 1] + vc[2, 2] + 2 * vc[1, 2])
# Critical value (e.g., 95% CI)
alpha <- 0.05
z_crit <- qnorm(1 - alpha / 2)
# Wald CIs
phi_CI   <- c(phi_hat - z_crit * se_phi,   phi_hat + z_crit * se_phi)
theta_CI <- c(theta_hat - z_crit * se_theta, theta_hat + z_crit * se_theta)
phi_hat
phi_CI
theta_hat
theta_CI
### Finally, we are ready to estimate recombinant counts
lineage_freq_all
# Get pipj for all i<j and all windows (using self-join with window id)
pairs_by_window <- lineage_freq_all %>%
filter(test_end <= as.Date("2023-03-19")) %>%
transmute(test_start, test_end, lineage = collapsed, p) %>%
group_by(test_start, test_end) %>%
mutate(.row = row_number()) %>%
ungroup() %>%                                         # <-- important: ungroup before self-join
inner_join(., ., by = c("test_start", "test_end"),
suffix = c("_i", "_j")) %>%
filter(.row_i < .row_j) %>%
transmute(
test_start, test_end,
i_raw  = lineage_i, j_raw  = lineage_j,
pi_raw = p_i,       pj_raw = p_j
) %>%
mutate(
i  = pmin(i_raw, j_raw),
j  = pmax(i_raw, j_raw),
pi = if_else(i_raw == i,  pi_raw, pj_raw),  # swap p’s if we swapped labels
pj = if_else(i_raw == i,  pj_raw, pi_raw),
pipj = pi * pj
) %>%
select(test_start, test_end, i, j, pi, pj, pipj)
# sanity check
test <- pairs_by_window %>% group_by(test_start) %>% summarize(sum(pipj))
# Attach theta, prev, and n
calculate_exp <- pairs_by_window %>% left_join(window_stats %>% select(test_start, test_end, n_samples, avg_prop))
calculate_exp$theta_hat <- theta_hat
# \hat E[R] = theta*n*prev*pi*pj (in each window)
calculate_exp$exp_hat <- calculate_exp$theta_hat * calculate_exp$n_samples * calculate_exp$avg_prop * calculate_exp$pipj
# sum across windows
calculate_exp_agg <- calculate_exp %>% group_by(i,j) %>% summarize(exp_hat = sum(exp_hat)) %>% arrange(desc(exp_hat))
colnames(calculate_exp_agg) <- c("parent1", "parent2", "exp")
### We have both obs. and exp. counts! Now join the two!
# How many sequences tested during ONS period?
res %>% filter(test_end <= as.Date("2023-03-19")) %>% nrow()
# Prep: normalize parents and apply date filter once
# temp has only two parental lineages
prep <- tmp %>%
mutate(
parent1 = pmin(pA, pB),
parent2 = pmax(pA, pB)
) %>%
filter(test_end <= as.Date("2023-03-19"))
# (1) Pair counts excluding 'other' (your original goal)
pair_counts_during_ons <- prep %>%
filter(parent1 != "other", parent2 != "other") %>%
count(parent1, parent2, name = "n_sequences") %>%
arrange(desc(n_sequences), parent1, parent2)
# pair_counts_during_ons_other <- prep %>%
#   count(parent1, parent2, name = "n_sequences") %>%
#   arrange(desc(n_sequences), parent1, parent2)
# (2) How many rows (within the date filter) have at least one 'other'
other_counts <- prep %>%
summarise(
n_total_in_range   = n(),
n_with_other       = sum(parent1 == "other" | parent2 == "other", na.rm = TRUE),
n_without_other    = sum(parent1 != "other" & parent2 != "other", na.rm = TRUE)
)
other_counts
sum(pair_counts_during_ons$n_sequences)
pair_counts_w_exp <- left_join(pair_counts_during_ons, calculate_exp_agg)
sum(pair_counts_w_exp$n_sequences)-sum(pair_counts_w_exp$exp)
sum(pair_counts_w_exp$exp)
sum(pair_counts_w_exp$n_sequences)
# Read and prepare ONS data
ons <- read.table("../data/ons_survey_data.tsv")
ons <- ons %>%
transmute(
day   = V1, month_name = V2, year = V3, percent = V4,
prop  = percent / 100,
date  = as.Date(paste(year, month_name, day, sep = "-"), format = "%Y-%B-%d")
)
setDT(windows)
windows[, `:=`(test_start = as.Date(test_start), test_end = as.Date(test_end))]
setDT(ons)
ons[, `:=`(start = date, end = date)]
setkey(ons, start, end)
setkey(windows, test_start, test_end)
ans <- foverlaps(
x = ons, y = windows,
by.x = c("start","end"),
by.y = c("test_start","test_end"),
type = "within",      # date within [test_start, test_end]
nomatch = 0L
)[
, .(avg_prop = mean(prop, na.rm = TRUE),
n_days  = .N),
by = .(test_start, test_end)
]
# testing
ons_second_win <- ons %>% filter(day >= 14, day <= 20, month_name == "September", year == 2020)
mean(ons_second_win$percent)
# proportion matches
summary_df <- summary_df %>%
left_join(
ans %>% select(test_start, ons_prop = avg_prop),
by = "test_start"
)
View(ans)
View(ans2)
View(ans)
