---
title: "inferred_vs_exp_new"
output: html_document
date: "2025-10-17"
---

```{r setup, include=FALSE}
library(tidyverse)
library(stringr)
library(readr)
library(purrr)
library(data.table)

# Inferred 
res <- read.csv("summary_files/combined_optimization_results_with_summary.csv") %>%
  mutate(across(c(test_start, test_end), as.Date)) %>%
  mutate(date = test_start + (test_end - test_start) / 2)
```

```{r}
# add number of parental lineages
res_w_n_parents <- res %>%
  mutate(
    n_parents = case_when(
      is.na(parental_lineages) | parental_lineages == "" ~ 0L,
      TRUE ~ str_count(parental_lineages, fixed(";")) + 1L
    )
  )

# counts
n_three_or_more <- sum(res_w_n_parents$n_parents >= 3, na.rm = TRUE)
n_one_only      <- sum(res_w_n_parents$n_parents == 1, na.rm = TRUE)

# filtered data: keep exactly 2 parental lineages
res_filtered <- res_w_n_parents %>%
  filter(n_parents == 2) %>%
  select(-n_parents)

# (optional) quick sanity table
tally_parents <- res_w_n_parents %>%
  count(n_parents, name = "n_rows") %>%
  arrange(n_parents)

# Compute numerator and denominator
num <- nrow(res_filtered)
den <- sum(tally_parents$n_rows[-1])

# Exact (Clopper–Pearson) binomial CI
bt <- binom.test(num, den)

bt$estimate   # estimated proportion
bt$conf.int   # exact 95% Clopper–Pearson CI
```
```{r}
# split parents (for those with two parental lineages)
tmp <- res_filtered %>%
  separate(parental_lineages, into = c("pA", "pB"), sep = ";", remove = FALSE)

# how many rows have "other" as a parent
n_with_other <- tmp %>%
  filter(str_to_lower(pA) == "other" | str_to_lower(pB) == "other") %>%
  nrow()

# drop rows with "other", canonicalize ordering, and tally
pair_counts <- tmp %>%
  filter(str_to_lower(pA) != "other", str_to_lower(pB) != "other") %>%
  mutate(parent1 = pmin(pA, pB), parent2 = pmax(pA, pB)) %>%
  count(parent1, parent2, name = "n_sequences") %>%
  arrange(desc(n_sequences), parent1, parent2)

# pair_counts has the number of detected recombinants with each unique parental lineage pair

# drop rows with "other", canonicalize ordering, and tally
pair_counts_w_other <- tmp %>%
  mutate(parent1 = pmin(pA, pB), parent2 = pmax(pA, pB)) %>%
  count(parent1, parent2, test_start, name = "n_sequences") %>%
  arrange(test_start, parent1, parent2)
```

```{r}
### Above, we already obtained the observed counts, so from here, we calculate expected counts

# Get number of samples and number of detected recombinants with two parental lineages in each window
res_pairs <- res %>%
  add_count(date, name = "n_samples") %>%          # <- adds n_samples per date
  filter(str_count(parental_lineages, ";") == 1) %>%
  group_by(date, test_start, test_end, n_samples) %>%                
  summarise(n_inferred = n(), .groups = "drop")

sum(res_pairs$n_inferred)

# Get test windows
windows <- res_pairs %>% select(test_start, test_end)

# Read and prepare ONS data 
ons <- read.table("../data/ons_survey_data.tsv")
ons <- ons %>%
  transmute(
    day   = V1, month_name = V2, year = V3, percent = V4,
    prop  = percent / 100,
    date  = as.Date(paste(year, month_name, day, sep = "-"), format = "%Y-%B-%d")
  )

# Expand windows to daily dates, join, and summarize

# The first four lines are left joining ons prevalence to expanded windows (now we have window and day as each row)
ans2 <- windows %>%
  mutate(date = map2(test_start, test_end, ~ seq(.x, .y, by = "day"))) %>%
  unnest(date) %>%
  left_join(ons, by = "date") %>%
  group_by(test_start, test_end) %>%
  summarise(
    avg_prop = mean(prop, na.rm = TRUE),
    n_days   = sum(!is.na(prop)),
    .groups  = "drop")
```

```{r}
### Above, we obtained window-averaged ONS estimates. Next, we need lineage proportions. 

# folder containing the files (adjust as needed)
dir_in <- "../run-on-cluster-3/real-data-analysis/output/sliding_windows/expected_recombinant_freq"

# regex for filenames like: lineage_freq_2024-02-19_2024-02-25.csv.gz
pat <- "^lineage_freq_(\\d{4}-\\d{2}-\\d{2})_(\\d{4}-\\d{2}-\\d{2})\\.csv\\.gz$"

file_df <- tibble(
  path = list.files(dir_in, pattern = pat, full.names = TRUE)) %>%
  mutate(
    fname      = basename(path),
    test_start = as.Date(str_match(fname, pat)[, 2]),
    test_end   = as.Date(str_match(fname, pat)[, 3]))

# (optional) sanity check
stopifnot(all(!is.na(file_df$test_start)), all(!is.na(file_df$test_end)))

lineage_freq_all <- file_df %>%
  select(path, test_start, test_end) %>%
  pmap_dfr(function(path, test_start, test_end) {
    read_csv(path, show_col_types = FALSE) %>%
      mutate(test_start = test_start,
             test_end   = test_end)})

# Result: one big data frame with the original columns + test_start/test_end
lineage_freq_all

# Here, we are obtaining sum (pipj) across all i<j, which is what we need to fit the linear regression later
pair_mass <- lineage_freq_all %>%                      
  group_by(test_start, test_end) %>%      
  summarise(
    s1 = sum(p, na.rm = TRUE),
    s2 = sum(p^2, na.rm = TRUE),
    sum_pipj = 0.5 * (s1^2 - s2),
    .groups = "drop"
  )

# Sanity check
pair_mass2 <- lineage_freq_all %>%
  group_by(test_start, test_end) %>%
  summarise(
    sum_pipj = {
      v <- p[!is.na(p)]
      if (length(v) < 2) 0 else sum(combn(v, 2, FUN = prod))
    },
    .groups = "drop"
  )

pair_mass
pair_mass2
```
```{r}
# test 
# pairs_coexist <- lineage_freq_all %>%
#   group_by(test_start) %>%
#   summarise(lineages = list(unique(collapsed)), .groups = "drop") 
# 
# lapply(pairs_coexist$lineages, function(x){"AY.9" %in% x & "B.1.617.2" %in% x})
```


```{r}
# Using lineage proportions + ONS prev estimates, we next estimate theta and phi using a linear regression.

# To the inferred number of recombinant sequences in each window, left join sum pipj in each window and the window-averaged ons prev.
window_stats <- res_pairs %>% left_join(pair_mass) %>% left_join(ans2)
# window_stats$denom <- window_stats$n_samples*window_stats$avg_prop*window_stats$sum_pipj
# window_stats$theta_first_model <- window_stats$n_inferred/window_stats$denom

# x_w = prev(w) sum pi(w)pj(w)
window_stats$x <- window_stats$avg_prop*window_stats$sum_pipj
# y_w = R^{total}(w)/n(w)
window_stats$y <- window_stats$n_inferred/window_stats$n_samples

window_stats <- window_stats[!is.na(window_stats$x),]

ggplot(window_stats, aes(x, y)) + geom_point()

# linear regression to est. theta and phi
lm <- lm(y~x, data = window_stats)
summary_lm <- summary(lm)
vc <- vcov(lm)
# Point estimates
phi_hat   <- summary_lm$coefficients[1, 1]
beta_hat  <- summary_lm$coefficients[2, 1]
theta_hat <- phi_hat + beta_hat

# Standard errors
se_phi   <- sqrt(vc[1, 1])
se_theta <- sqrt(vc[1, 1] + vc[2, 2] + 2 * vc[1, 2])

# Critical value (e.g., 95% CI)
alpha <- 0.05
z_crit <- qnorm(1 - alpha / 2)

# Wald CIs
phi_CI   <- c(phi_hat - z_crit * se_phi,   phi_hat + z_crit * se_phi)
theta_CI <- c(theta_hat - z_crit * se_theta, theta_hat + z_crit * se_theta)

phi_hat
phi_CI

theta_hat
theta_CI
```

```{r}
### Finally, we are ready to estimate recombinant counts

lineage_freq_all

# Get pipj for all i<j and all windows (using self-join with window id)
pairs_by_window <- lineage_freq_all %>%
  filter(test_end <= as.Date("2023-03-19")) %>%
  transmute(test_start, test_end, lineage = collapsed, p) %>%
  group_by(test_start, test_end) %>%
  mutate(.row = row_number()) %>%
  ungroup() %>%                                         # <-- important: ungroup before self-join
  inner_join(., ., by = c("test_start", "test_end"),
             suffix = c("_i", "_j")) %>%
  filter(.row_i < .row_j) %>%
  transmute(
    test_start, test_end,
    i_raw  = lineage_i, j_raw  = lineage_j,
    pi_raw = p_i,       pj_raw = p_j
  ) %>%
  mutate(
    i  = pmin(i_raw, j_raw),
    j  = pmax(i_raw, j_raw),
    pi = if_else(i_raw == i,  pi_raw, pj_raw),  # swap p’s if we swapped labels
    pj = if_else(i_raw == i,  pj_raw, pi_raw),
    pipj = pi * pj
  ) %>%
  select(test_start, test_end, i, j, pi, pj, pipj)

# Attach theta, prev, and n
calculate_exp <- pairs_by_window %>% left_join(window_stats %>% select(test_start, test_end, n_samples, avg_prop))
calculate_exp$theta_hat <- theta_hat

# \hat E[R] = theta*n*prev*pi*pj (in each window)
calculate_exp$exp_hat <- calculate_exp$theta_hat * calculate_exp$n_samples * calculate_exp$avg_prop * calculate_exp$pipj
# sum across windows
calculate_exp_agg <- calculate_exp %>% group_by(i,j) %>% summarize(exp_hat = sum(exp_hat)) %>% arrange(desc(exp_hat))

colnames(calculate_exp_agg) <- c("parent1", "parent2", "exp")
```

```{r}
### We have both obs. and exp. counts! Now join the two!

# How many sequences tested during ONS period?
res %>% filter(test_end <= as.Date("2023-03-19")) %>% nrow()

# Prep: normalize parents and apply date filter once
# temp has only two parental lineages
prep <- tmp %>%
  mutate(
    parent1 = pmin(pA, pB),
    parent2 = pmax(pA, pB)
  ) %>%
  filter(test_end <= as.Date("2023-03-19"))

# (1) Pair counts excluding 'other' (your original goal)
pair_counts_during_ons <- prep %>%
  filter(parent1 != "other", parent2 != "other") %>%
  count(parent1, parent2, name = "n_sequences") %>%
  arrange(desc(n_sequences), parent1, parent2)

# (2) How many rows (within the date filter) have at least one 'other'
other_counts <- prep %>%
  summarise(
    n_total_in_range   = n(),
    n_with_other       = sum(parent1 == "other" | parent2 == "other", na.rm = TRUE),
    n_without_other    = sum(parent1 != "other" & parent2 != "other", na.rm = TRUE)
  )
other_counts

sum(pair_counts_during_ons$n_sequences)

pair_counts_w_exp <- left_join(pair_counts_during_ons, calculate_exp_agg)

sum(pair_counts_w_exp$n_sequences)-sum(pair_counts_w_exp$exp)
```


```{r}
library(dplyr)
library(ggplot2)
library(scales)
library(ggrepel)

#--- Data + model --------------------------------------------------------------
# Assumes pair_counts_w_exp has columns: n_sequences (x), exp (y),
# and parental pair identifiers parent1, parent2

# Fit OLS and get residuals
fit <- lm(n_sequences ~ exp, data = pair_counts_w_exp)

summary(fit)

df_with_res <- pair_counts_w_exp %>%
  mutate(
    .resid    = resid(fit),
    .stdresid = rstandard(fit),
    label     = paste0(parent1, " - ", parent2)
  )

# Which points to label? (|standardized residual| >= 3)
pairs_to_label <- df_with_res %>%
  arrange(desc(abs(.resid))) %>%   # sort by extremeness
  slice_head(n = 20)           

#--- Correlation test (Pearson, complete cases) --------------------------------
ct <- with(pair_counts_w_exp,
           cor.test(n_sequences, exp, use = "complete.obs", method = "pearson"))

r_val <- unname(ct$estimate)
p_val <- ct$p.value
n_obs <- sum(complete.cases(pair_counts_w_exp[, c("n_sequences", "exp")]))

cap_text <- sprintf("Pearson correlation r = %.3f (p = %.3g), n = %d", r_val, p_val, n_obs)

#--- Plot ----------------------------------------------------------------------
p_pairs <- ggplot(pair_counts_w_exp, aes(x = exp, y = n_sequences)) +
  geom_point(alpha = 0.7, size = 1.9, stroke = 0) +
  geom_abline(slope = 1) +
  # Least-squares line with 95% CI ribbon
  geom_smooth(method = "lm", se = TRUE, linewidth = 0.8) +
  scale_x_continuous(labels = label_number(accuracy = 1), expand = expansion(mult = c(0.02, 0.05))) +
  scale_y_continuous(labels = label_number(accuracy = 1), expand = expansion(mult = c(0.02, 0.05))) +
  labs(
    y = "Detected recombinants",
    x = "Expected true positive recombinants",
    caption = cap_text
  ) +
  theme_bw(base_size = 15) +
  theme(
    legend.position = "none",
    plot.caption = element_text(hjust = 0)
  )

# Add labels for large standardized residuals (if any)
p_pairs_lab <- p_pairs +
  geom_text_repel(
    data = pairs_to_label,
    aes(label = label),
    size = 3.2,
    max.overlaps = Inf,
    box.padding = 0.25,
    point.padding = 0.2,
    min.segment.length = 0,
    seed = 42
  )

# Draw & save
p_pairs_lab
ggsave("figs/pairs_scatter_detected_vs_expected.png",
       p_pairs_lab, width = 6, height = 6, dpi = 300)

```
```{r}
# Which points to label? (|standardized residual| >= 3)
label2 <- df_with_res %>%
  arrange(desc(abs(.resid))) %>%   # sort by extremeness
  slice_head(n = 20) %>% filter(.stdresid < 0)
```

```{r}
library(scales)

# --- define pairs of interest ---
pairs_tbl <- tibble::tibble(
  i = c("BA.1","BA.1","BA.1.1","BQ.1.1","BA.1.1","CH.1.1"),
  j = c("BA.1.1","BA.1.17.2","BA.1.17.2","CH.1.1","BA.2","XBB.1.5")
)

pairs_keys <- pairs_tbl %>%
  mutate(key = paste(pmin(i, j), pmax(i, j), sep = "-")) %>%
  distinct(key) %>%
  pull(key)

# --- normalize pairs in your data and filter ---
use_plot <- calculate_exp %>%
  mutate(
    key  = paste(pmin(i, j), pmax(i, j), sep = "-"),
    pair = key,
    mid_date = test_start + (test_end - test_start) / 2  # midpoint of window
  ) %>%
  filter(key %in% pairs_keys)

# --- Plot trajectories of expected counts over time -------------
p_expected <- ggplot(use_plot, aes(x = mid_date, y = exp_hat, color = pair)) +
  geom_line(linewidth = 0.8, alpha = 0.9) +
  geom_point(size = 1.9, alpha = 0.8, stroke = 0) +
  scale_x_date(
    date_breaks = "2 months",
    date_labels = "%b %Y",
    expand = expansion(mult = c(0.02, 0.05))
  ) +
  scale_y_continuous(
    labels = scales::label_scientific(digits = 2),
    expand = expansion(mult = c(0.02, 0.05))
  ) +
  labs(
    x = "Date",
    y = "Expected true positives",
    color = "Parental lineage pair"
  ) +
  theme_bw(base_size = 15) +
  theme(
    plot.caption = element_text(hjust = 0),
    legend.position = "right",
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

p_expected

ggsave("figs/expected_trajectories_selected_pairs.png",
       p_expected, width = 8, height = 4, dpi = 300)

```


```{r}
# library(ggrepel)
# 
# # Fit and compute residuals
# fit <- lm(exp ~ n_sequences, data = pair_counts_w_exp)
# res_df <- tibble(
#   .resid     = resid(fit),
#   .stdresid  = rstandard(fit)
# )
# 
# # Bind residuals back to your data (row order preserved)
# df_with_res <- bind_cols(pair_counts_w_exp, res_df) %>%
#   mutate(label = paste0(parent1, " - ", parent2))
# 
# ## Option A: label all with large standardized residuals (|z| >= 2)
# pairs_to_label <- df_with_res %>%
#   filter(abs(.stdresid) >= 3)
# 
# p_pairs <- ggplot(pair_counts_w_exp, aes(x = n_sequences, y = exp)) + geom_point()
# 
# # Add labels to your plot
# p_pairs_lab <- p_pairs +
#   geom_text_repel(
#     data = pairs_to_label,
#     aes(label = label),
#     size = 3.2,
#     max.overlaps = Inf,
#     box.padding = 0.25,
#     point.padding = 0.2,
#     min.segment.length = 0,
#     seed = 42
#   )
# 
# p_pairs_lab
# ggsave("figs/pairs_scatter_detected_vs_expected.png",
#        p_pairs_lab, width = 6, height = 6, dpi = 300)

```
```{r}
# library(dplyr)
# library(ggplot2)
# library(scales)
# 
# # flag + combo label
# plot_df <- pair_counts_w_exp %>%
#   mutate(
#     is_flag = n_sequences < 100 & exp > 25,
#     combo   = paste(parent1, parent2, sep = " - ")
#   )
# 
# flagged_df <- plot_df %>% filter(is_flag)
# others_df  <- plot_df %>% filter(!is_flag)
# 
# # palette for flagged combos
# flag_combos <- flagged_df$combo %>% unique()
# pal_flagged <- scales::hue_pal()(length(flag_combos))
# names(pal_flagged) <- flag_combos
# 
# # build plot
# p_pairs_col <- ggplot() +
#   # clearer "Other" points (slightly darker gray + higher alpha)
#   geom_point(
#     data = others_df,
#     aes(x = n_sequences, y = exp),
#     color = "grey60", alpha = 0.55, size = 1.8, stroke = 0
#   ) +
#   # flagged combos with distinct colors + legend
#   geom_point(
#     data = flagged_df,
#     aes(x = n_sequences, y = exp, color = combo),
#     alpha = 0.9, size = 2.1, stroke = 0
#   ) +
#   # least-squares fit over ALL points
#   geom_smooth(
#     data = plot_df,
#     aes(x = n_sequences, y = exp),
#     method = "lm", se = FALSE, linewidth = 0.9, color = "black"
#   ) +
#   scale_x_continuous(labels = label_number(accuracy = 1),
#                      expand = expansion(mult = c(0, 0.01))) +
#   scale_y_continuous(labels = label_number(accuracy = 1),
#                      expand = expansion(mult = c(0, 0.01))) +
#   scale_color_manual(values = pal_flagged, name = "Parental lineage pair") +
#   labs(
#     x = "Detected recombinant count (R_{i,j})",
#     y = "Expected count (\\hat{E}[R_{i,j}])"
#   ) +
#   theme_bw(base_size = 16) +
#   theme(
#     legend.position = "right",
#     axis.text.x = element_text(angle = 45, hjust = 1)
#   )
# 
# p_pairs_col
# ggsave("figs/pairs_scatter_detected_vs_expected.png", p_pairs_col,
#        width = 6, height = 6, dpi = 300)

```

```{r}
# pair_counts_w_exp %>% arrange(desc(exp/n_sequences))
# nrow(res)*phi_hat
```
```{r}
# cor(pair_counts_w_exp$n_sequences, pair_counts_w_exp$exp, method = "pearson")
# cor.test(pair_counts_w_exp$n_sequences, pair_counts_w_exp$exp, method = "pearson")
```

```{r}
# # Packages
# library(dplyr)
# library(forcats)
# library(ggplot2)
# library(ggstream)
# library(scales)
# 
# pairs_to_label_pos <- pairs_to_label %>% filter(.stdresid > 3)
# highlights <- unique(c(pairs_to_label_pos$parent1, pairs_to_label_pos$parent2))
# lineage_freq_all$collapsed_other <- ifelse(lineage_freq_all$collapsed %in% highlights, lineage_freq_all$collapsed, "other")
# 
# lineage_freq_all_collapsed_other <- lineage_freq_all %>% group_by(test_start, test_end, collapsed_other) %>% summarize(count = sum(count))
# 
# ggplot(lineage_freq_all_collapsed_other %>% filter(test_end <= as.Date("2022-08-31"), test_end >= as.Date("2021-05-31")), aes(x = test_end, y = count, fill = collapsed_other)) +
#   # type = "proportional" converts counts to proportions at each date
#   geom_stream(type = "proportional") +
#   scale_y_continuous(labels = percent_format(accuracy = 1)) +
#   scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") +
#   labs(
#     x = NULL,
#     y = "Proportion",
#     fill = "Lineage",
#     title = "Weekly lineage proportions",
#     subtitle = "Through 2023-03-31"
#   ) +
#   theme_minimal(base_size = 12) +
#   theme(
#     panel.grid.minor = element_blank(),
#     legend.position = "right"
#   ) + theme(
#     axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
#   )
```



